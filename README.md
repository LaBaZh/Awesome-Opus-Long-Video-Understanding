# Awesome Long Video Understanding: [![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)
A curated list of real long-form video understanding resources, inspired by [awesome-computer-vision](https://github.com/jbhuang0604/awesome-computer-vision).

## Contents
 - [Dataset](#Dataset)
   - [Long-form video dataset](#Long-form-video-dataset)
   - [SOTA Result](#SOTA-Result)
 - [Survey](#Related-Survey)
 - [Papers & Codes](#Papers-&-Codes)
 - [Miscellaneous](#Miscellaneous)

## Dataset
### Video-Text Dataset
* [VIOLIN](https://github.com/jimmy646/violin)
* [ShareGPT4Video](https://sharegpt4video.github.io/)
* [Panda-70](https://github.com/snap-research/Panda-70M)

### Long-form video dataset
* [MovieChat](https://github.com/rese1f/MovieChat)
* [VidChapter](https://antoyang.github.io/vidchapters.html)
* [MovieNet](https://movienet.github.io/)
* [EgoSchema](https://egoschema.github.io/)
* [NeXT-QA](https://github.com/doc-doc/NExT-QA)

### Video Editing Dataset
* [TGT](http://arxiv.org/abs/2404.03477)
* [AVE](https://github.com/dawitmureja/AVE)
* [MovieCuts](https://github.com/PardoAlejo/MovieCuts)
* [MatchCutting](https://github.com/netflix/matchcut)
* [Edit3K](http://arxiv.org/abs/2403.16048)
* [EditVid-QA](http://arxiv.org/abs/2406.10484)
* [AutoTransition](https://github.com/acherstyx/AutoTransition)
* [AudioMatchCutting](https://ieeexplore.ieee.org/abstract/document/10447306)
* 

### Movie-related video dataset
* [MovieQA](http://arxiv.org/abs/1512.02902)
* [TVSum](https://github.com/yalesong/tvsum)
* [SFD](https://shortfilmdataset.github.io/)
* [MovieGraph](http://moviegraphs.cs.toronto.edu/)
* [MAD](https://github.com/Soldelli/MAD)
* [Trailer](http://arxiv.org/abs/2008.08502)
* [Learn to Cuts](https://www.alejandropardo.net/publication/learning-to-cut/)
* [Movie Description Dataset](http://arxiv.org/abs/1501.02530)

### Video Understanding Evaluation Benchmark
* [VBench](https://vchitect.github.io/VBench-project/)
* [MVBench](https://github.com/OpenGVLab/Ask-Anything/blob/main/video_chat2/MVBENCH.md)
* [Vript](https://github.com/mutonix/Vript)
* [ViP-Bench](https://github.com/mutonix/Vript)
* [VideoVista](http://arxiv.org/abs/2406.11303)
* [Video-MME](http://arxiv.org/abs/2405.21075)
* [Video-Bench](https://github.com/PKU-YuanGroup/Video-Bench)
* [Event-Bench](https://github.com/RUCAIBox/Event-Bench)
* [TempCompass](https://github.com/llyx97/TempCompass)
* [Needle In A Video Haystack](https://github.com/joez17/VideoNIAH)
* [MMBench-Video](https://github.com/open-compass/VLMEvalKit)
* [MLVU](https://github.com/FlagOpen/FlagEmbedding/tree/master/MLVU)
* [LVBench](https://lvbench.github.io/)
* 

### SOTA Result
ðŸ”¨TODO: Collect SOTA results of different datasets and merge to one table

## Related Survey
- [Video Understanding with Large Language Models: A Survey](https://arxiv.org/pdf/2312.17432)

## Papers & Codes
ðŸ”¨TODO: Categorize the papers and works.

## Miscellaneous
- [video2dataset](https://github.com/iejMac/video2dataset): video dataset construction and download tools.

## Contributing
Please read the [contribution guidelines](contributing.md). Then please feel free to send me [pull requests]([https://github.com/LaBaZh/Awesome-Long-Video-Understanding/pull]) or email (lee.li@opus.pro) to add links.
